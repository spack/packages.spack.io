{
    "aliases": [],
    "build_system": "PythonPackage",
    "conflicts": [],
    "dependencies": [
        {
            "description": "HTML parser based on the WHATWG HTML specification.",
            "name": "py-html5lib"
        },
        {
            "description": "The PyPA recommended tool for installing Python packages.",
            "name": "py-pip"
        },
        {
            "description": "A Python utility that aids in the process of downloading, building,\nupgrading, installing, and uninstalling Python packages.",
            "name": "py-setuptools"
        },
        {
            "description": "Measures number of Terminal column cells of wide-character codes",
            "name": "py-wcwidth"
        },
        {
            "description": "A built-package format for Python.",
            "name": "py-wheel"
        },
        {
            "description": "The Python programming language.",
            "name": "python"
        },
        {
            "description": "A Spack managed Python virtual environment",
            "name": "python-venv"
        }
    ],
    "dependent_to": [
        {
            "description": "User-generated content on the Web and in social media is often dirty.\nPreprocess your scraped data with clean-text to create a normalized text\nrepresentation.",
            "name": "py-clean-text"
        },
        {
            "description": "CLIP (Contrastive Language-Image Pre-Training) is a neural network\ntrained on a variety of (image, text) pairs. It can be instructed in\nnatural language to predict the most relevant text snippet, given an\nimage, without directly optimizing for the task, similarly to the zero-\nshot capabilities of GPT-2 and 3. We found CLIP matches the performance\nof the original ResNet50 on ImageNet \"zero-shot\" without using any of\nthe original 1.28M labeled examples, overcoming several major challenges\nin computer vision.",
            "name": "py-clip-anytorch"
        },
        {
            "description": "Welcome to an open source implementation of OpenAI's CLIP (Contrastive\nLanguage-Image Pre-training).",
            "name": "py-open-clip-torch"
        },
        {
            "description": "A dict subclass with keylist/keypath support, I/O shortcuts and many\nutilities.",
            "name": "py-python-benedict"
        },
        {
            "description": "A concise but complete implementation of CLIP with various experimental\nimprovements from recent papers",
            "name": "py-x-clip"
        }
    ],
    "description": "Fixes Unicode that's broken in various ways.\n",
    "homepage": "https://ftfy.readthedocs.io",
    "latest_version": "6.0.3",
    "maintainers": [],
    "name": "py-ftfy",
    "patches": [],
    "resources": [],
    "variants": [
        {
            "default": "python_pip",
            "description": "Build systems supported by the package",
            "name": "build_system"
        }
    ],
    "versions": [
        {
            "name": "6.0.3",
            "sha256": "ba71121a9c8d7790d3e833c6c1021143f3e5c4118293ec3afb5d43ed9ca8e72b"
        },
        {
            "name": "5.8",
            "sha256": "51c7767f8c4b47d291fcef30b9625fb5341c06a31e6a3b627039c706c42f3720"
        },
        {
            "name": "4.4.3",
            "sha256": "3c0066db64a98436e751e56414f03f1cdea54f29364c0632c141c36cca6a5d94"
        }
    ],
    "versions_deprecated": []
}