{
    "aliases": [],
    "build_system": "RPackage",
    "conflicts": [],
    "dependencies": [
        {
            "description": "R is 'GNU S', a freely available language and environment for\nstatistical computing and graphics which provides a wide variety of\nstatistical and graphical techniques: linear and nonlinear modelling,\nstatistical tests, time series analysis, classification, clustering,\netc. Please consult the R project homepage for further information.",
            "name": "r"
        },
        {
            "description": "Applies Multiclass AdaBoost.M1, SAMME and Bagging. It implements Freund\nand Schapire's Adaboost.M1 algorithm and Breiman's Bagging algorithm\nusing classification trees as individual classifiers. Once these\nclassifiers have been trained, they can be used to predict on new data.\nAlso, cross validation estimation of the error can be done. Since\nversion 2.0 the function margins() is available to calculate the margins\nfor these classifiers. Also a higher flexibility is achieved giving\naccess to the rpart.control() argument of 'rpart'. Four important new\nfeatures were introduced on version 3.0, AdaBoost-SAMME (Zhu et al.,\n2009) is implemented and a new function errorevol() shows the error of\nthe ensembles as a function of the number of iterations. In addition,\nthe ensembles can be pruned using the option 'newmfinal' in the\npredict.bagging() and predict.boosting() functions and the posterior\nprobability of each class for observations can be obtained. Version 3.1\nmodifies the relative importance measure to take into account the gain\nof the Gini index given by a variable in each tree and the weights of\nthese trees. Version 4.0 includes the margin-based ordered aggregation\nfor Bagging pruning (Guo and Boukir, 2013) and a function to auto prune\nthe 'rpart' tree. Moreover, three new plots are also available\nimportanceplot(), plot.errorevol() and plot.margins(). Version 4.1\nallows to predict on unlabeled data. Version 4.2 includes the parallel\ncomputation option for some of the functions.",
            "name": "r-adabag"
        },
        {
            "description": "Rule- And Instance-Based Regression Modeling. Regression modeling using\nrules with added instance-based corrections.",
            "name": "r-cubist"
        },
        {
            "description": "Misc Functions of the Department of Statistics, Probability Theory Group\n(Formerly: E1071), TU Wien. Functions for latent class analysis, short\ntime Fourier transform, fuzzy clustering, support vector machines,\nshortest path computation, bagged clustering, naive Bayes classifier,\ngeneralized k-nearest neighbour ...",
            "name": "r-e1071"
        },
        {
            "description": "Lasso and Elastic-Net Regularized Generalized Linear Models. Extremely\nefficient procedures for fitting the entire lasso or elastic-net\nregularization path for linear regression, logistic and multinomial\nregression models, Poisson regression and the Cox model. Two recent\nadditions are the multiple-response Gaussian, and the grouped\nmultinomial. The algorithm uses cyclical coordinate descent in a path-\nwise fashion, as described in the paper linked to via the URL below.",
            "name": "r-glmnet"
        },
        {
            "description": "Kernel-Based Machine Learning Lab. Kernel-based machine learning methods\nfor classification, regression, clustering, novelty detection, quantile\nregression and dimensionality reduction. Among other methods 'kernlab'\nincludes Support Vector Machines, Spectral Clustering, Kernel PCA,\nGaussian Processes and a QP solver.",
            "name": "r-kernlab"
        },
        {
            "description": "Weighted k-Nearest Neighbors. Weighted k-Nearest Neighbors for\nClassification, Regression and Clustering.",
            "name": "r-kknn"
        },
        {
            "description": "Trellis Graphics for R. A powerful and elegant high-level data\nvisualization system inspired by Trellis graphics, with an emphasis on\nmultivariate data. Lattice is sufficient for typical graphics needs, and\nis also flexible enough to handle most nonstandard requirements. See\n?Lattice for an introduction.",
            "name": "r-lattice"
        },
        {
            "description": "Support Functions and Datasets for Venables and Ripley's MASS. Functions\nand datasets to support Venables and Ripley, \"Modern Applied Statistics\nwith S\" (4th edition, 2002).",
            "name": "r-mass"
        },
        {
            "description": "Mixture and Flexible Discriminant Analysis. Mixture and flexible\ndiscriminant analysis, multivariate adaptive regression splines (MARS),\nBRUTO.",
            "name": "r-mda"
        },
        {
            "description": "Feed-Forward Neural Networks and Multinomial Log-Linear Models. Software\nfor feed-forward neural networks with a single hidden layer, and for\nmultinomial log-linear models.",
            "name": "r-nnet"
        },
        {
            "description": "A Laboratory for Recursive Partytioning. A computational toolbox for\nrecursive partitioning. The core of the package is ctree(), an\nimplementation of conditional inference trees which embed tree-\nstructured regression models into a well defined theory of conditional\ninference procedures. This non-parametric class of regression trees is\napplicable to all kinds of regression problems, including nominal,\nordinal, numeric, censored as well as multivariate response variables\nand arbitrary measurement scales of the covariates. Based on conditional\ninference trees, cforest() provides an implementation of Breiman's\nrandom forests. The function mob() implements an algorithm for recursive\npartitioning based on parametric models (e.g. linear models, GLMs or\nsurvival regression) employing parameter instability tests for split\nselection. Extensible functionality for visualizing tree-structured\nregression models is available. The methods are described in Hothorn et\nal. (2006) <doi:10.1198/106186006X133933>, Zeileis et al. (2008)\n<doi:10.1198/106186008X319331> and Strobl et al. (2007)\n<doi:10.1186/1471-2105-8-25>.",
            "name": "r-party"
        },
        {
            "description": "Various Plotting Functions. Lots of plots, various labeling, axis and\ncolor scaling functions.",
            "name": "r-plotrix"
        },
        {
            "description": "Partial Least Squares and Principal Component Regression. Multivariate\nregression methods Partial Least Squares Regression (PLSR), Principal\nComponent Regression (PCR) and Canonical Powered Partial Least Squares\n(CPPLS).",
            "name": "r-pls"
        },
        {
            "description": "Breiman and Cutler's Random Forests for Classification and Regression.\nClassification and regression based on a forest of trees using random\ninputs.",
            "name": "r-randomforest"
        },
        {
            "description": "Recursive Partitioning and Regression Trees. Recursive partitioning for\nclassification, regression and survival trees. An implementation of most\nof the functionality of the 1984 book by Breiman, Friedman, Olshen and\nStone.",
            "name": "r-rpart"
        },
        {
            "description": "Extreme Gradient Boosting. Extreme Gradient Boosting, which is an\nefficient implementation of gradient boosting framework. This package is\nits R interface. The package includes efficient linear model solver and\ntree learning algorithms. The package can automatically do parallel\ncomputation on a single machine which could be more than 10 times faster\nthan existing gradient boosting packages. It supports various objective\nfunctions, including regression, classification and ranking. The package\nis made to be extensible, so that users are also allowed to define their\nown objectives easily.",
            "name": "r-xgboost"
        }
    ],
    "dependent_to": [
        {
            "description": "Condition-Dependent Operon Predictions. An implementation of the\ncomputational strategy for the comprehensive analysis of condition-\ndependent operon maps in prokaryotes proposed by Fortino et al. (2014)\n<doi:10.1186/1471-2105-15-145>. It uses RNA-seq transcriptome profiles\nto improve prokaryotic operon map inference.",
            "name": "r-condop"
        }
    ],
    "description": "Data Mining Classification and Regression Methods. Facilitates the use\nof data mining algorithms in classification and regression (including\ntime series forecasting) tasks by presenting a short and coherent set of\nfunctions. Versions: 1.4.6 / 1.4.5 / 1.4.4 new automated machine\nlearning (AutoML) and ensembles, via improved fit(), mining() and\nmparheuristic() functions, and new categorical preprocessing, via\nimproved delevels() function; 1.4.3 new metrics (e.g., macro precision,\nexplained variance), new \"lssvm\" model and improved mparheuristic()\nfunction; 1.4.2 new \"NMAE\" metric, \"xgboost\" and \"cv.glmnet\" models (16\nclassification and 18 regression models); 1.4.1 new tutorial and more\nrobust version; 1.4 - new classification and regression models, with a\ntotal of 14 classification and 15 regression methods, including:\nDecision Trees, Neural Networks, Support Vector Machines, Random\nForests, Bagging and Boosting; 1.3 and 1.3.1 - new classification and\nregression metrics; 1.2 - new input importance methods via improved\nImportance() function; 1.0 - first version.\n",
    "homepage": "https://cloud.r-project.org/package=rminer",
    "latest_version": "1.4.6",
    "maintainers": [],
    "name": "r-rminer",
    "patches": [],
    "resources": [],
    "variants": [
        {
            "default": "generic",
            "description": "Build systems supported by the package",
            "name": "build_system"
        }
    ],
    "versions": [
        {
            "name": "1.4.6",
            "sha256": "1f8bf7b3fbc887fd766568c1ec1f861021c962259354bd8967a61c1d0761cdf7"
        },
        {
            "name": "1.4.2",
            "sha256": "64444dcedcd17f2f26129819d6bd2f84d4bb59c8f65328b6054ef32cb9624fc2"
        }
    ],
    "versions_deprecated": []
}